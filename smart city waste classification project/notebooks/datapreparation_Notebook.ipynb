{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ba687a-99cb-4287-b70e-59330c72338b",
   "metadata": {},
   "source": [
    "## **orginal dataset- splitting script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbd67d3a-85b5-46f9-a2ef-4edbbda340a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è Dataset split skipped (script runned already, this script was just a reference to see how script's split done.).\n"
     ]
    }
   ],
   "source": [
    "RUN_DATASET_SPLIT = False \n",
    "\n",
    "if RUN_DATASET_SPLIT:\n",
    "    import os\n",
    "    import random\n",
    "\n",
    "    BASE_DIR = \"  \" #  \" enter your dataset - folder name that conatins train,val,test folders\". here mine is \" i.e dataset \"\n",
    "\n",
    "    splits = {\n",
    "    \"train\": 0.70,\n",
    "    \"val\": 0.15,\n",
    "    \"test\": 0.15\n",
    "    }\n",
    "\n",
    "# Get class names\n",
    "    classes = os.listdir(os.path.join(BASE_DIR, \"train\"))\n",
    "    classes = [c for c in classes if not c.startswith('.')]\n",
    "\n",
    "    for cls in classes:\n",
    "        print(f\"Processing class: {cls}\")\n",
    "\n",
    "    # Take image list from train folder\n",
    "        train_path = os.path.join(BASE_DIR, \"train\", cls)\n",
    "        images = [img for img in os.listdir(train_path)\n",
    "                  if img.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "\n",
    "        random.shuffle(images)\n",
    "\n",
    "        total = len(images)\n",
    "        train_end = int(total * splits[\"train\"])\n",
    "        val_end = train_end + int(total * splits[\"val\"])\n",
    "\n",
    "        keep = {\n",
    "            \"train\": set(images[:train_end]),\n",
    "            \"val\": set(images[train_end:val_end]),\n",
    "            \"test\": set(images[val_end:])\n",
    "        }\n",
    "\n",
    "    # Clean each split\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            split_path = os.path.join(BASE_DIR, split, cls)\n",
    "            for img in os.listdir(split_path):\n",
    "                if img not in keep[split]:\n",
    "                    os.remove(os.path.join(split_path, img))\n",
    "\n",
    "    print(\"‚úÖ Dataset split completed successfully!\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Dataset split skipped (script runned already, this script was just a reference to see how script's split done.).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c2b4d7-0110-404c-8b0f-8a664d5f94be",
   "metadata": {},
   "source": [
    "when we run the script mac will load hidden files i.e, **.DS_store files** which must be removed so i done this below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd45da21-dae2-4c0a-a8f0-ed1484ca858a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed: dataset/.DS_Store\n",
      "Removed: dataset/test/.DS_Store\n",
      "Removed: dataset/train/.DS_Store\n",
      "Removed: dataset/val/.DS_Store\n",
      "\n",
      "‚úÖ Total .DS_Store files removed: 4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def remove_ds_store(root_dir):\n",
    "    removed = 0\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        if \".DS_Store\" in files:\n",
    "            ds_path = os.path.join(root, \".DS_Store\")\n",
    "            os.remove(ds_path)\n",
    "            removed += 1\n",
    "            print(f\"Removed: {ds_path}\")\n",
    "    print(f\"\\n‚úÖ Total .DS_Store files removed: {removed}\")\n",
    "\n",
    "remove_ds_store(\"dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29aa8fc7-0b3c-410d-8552-c6719f7bf102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN\n",
      "paper: 597\n",
      "textile: 689\n",
      "metal: 630\n",
      "cardboard: 625\n",
      "organic: 676\n",
      "e-waste: 695\n",
      "glass: 663\n",
      "plastic: 623\n",
      "\n",
      "VAL\n",
      "paper: 127\n",
      "textile: 147\n",
      "metal: 135\n",
      "cardboard: 133\n",
      "organic: 145\n",
      "e-waste: 148\n",
      "glass: 142\n",
      "plastic: 133\n",
      "\n",
      "TEST\n",
      "paper: 129\n",
      "textile: 149\n",
      "metal: 136\n",
      "cardboard: 135\n",
      "organic: 146\n",
      "e-waste: 150\n",
      "glass: 143\n",
      "plastic: 135\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base = \"dataset\"\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    print(f\"\\n{split.upper()}\")\n",
    "    split_path = os.path.join(base, split)\n",
    "\n",
    "    for cls in os.listdir(split_path):\n",
    "        cls_path = os.path.join(split_path, cls)\n",
    "\n",
    "        if not os.path.isdir(cls_path):\n",
    "            continue  # skip files like .DS_Store\n",
    "\n",
    "        count = len([\n",
    "            f for f in os.listdir(cls_path)\n",
    "            if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "        ])\n",
    "\n",
    "        print(f\"{cls}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b25ce6-32a3-4138-9ba6-472e5246a298",
   "metadata": {},
   "source": [
    "## **binary classification**- recyclable/non-recyclable dataset\n",
    "This script will:\n",
    "- ‚úî Read from your existing dataset/\n",
    "- ‚úî Create a new binary dataset\n",
    "- ‚úî Keep train/val/test splits intact\n",
    "\n",
    "**üìÅ Output structure**\n",
    "\n",
    "- binary_dataset/\n",
    "-   ‚îú‚îÄ‚îÄ train/\n",
    "-   ‚îÇ   ‚îú‚îÄ‚îÄ recyclable/\n",
    "-   ‚îÇ   ‚îî‚îÄ‚îÄ non_recyclable/\n",
    "-   ‚îú‚îÄ‚îÄ val/\n",
    "-   ‚îî‚îÄ‚îÄ test/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "074cd9fc-61cf-4df5-b58d-8b16416dcc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è Dataset split skipped (script runned already and saved to disk\n"
     ]
    }
   ],
   "source": [
    "RUN_DATASET_SPLIT = False \n",
    "\n",
    "if RUN_DATASET_SPLIT:\n",
    "    import os\n",
    "    import shutil\n",
    "\n",
    "    SOURCE_DIR = \"dataset\"\n",
    "    TARGET_DIR = \"binary_dataset\"\n",
    "\n",
    "    recyclable_classes = [\n",
    "        \"paper\", \"plastic\", \"glass\",\n",
    "        \"metal\", \"cardboard\", \"e-waste\"\n",
    "    ]\n",
    "\n",
    "    non_recyclable_classes = [\n",
    "        \"organic\", \"textile\"\n",
    "    ]\n",
    "\n",
    "    splits = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "    for split in splits:\n",
    "        for label in [\"recyclable\", \"non_recyclable\"]:\n",
    "            os.makedirs(os.path.join(TARGET_DIR, split, label), exist_ok=True)\n",
    "\n",
    "    for split in splits:\n",
    "        for cls in os.listdir(os.path.join(SOURCE_DIR, split)):\n",
    "            cls_path = os.path.join(SOURCE_DIR, split, cls)\n",
    "            if not os.path.isdir(cls_path):\n",
    "                continue\n",
    "\n",
    "            if cls in recyclable_classes:\n",
    "                target_label = \"recyclable\"\n",
    "            elif cls in non_recyclable_classes:\n",
    "                target_label = \"non_recyclable\"\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            for img in os.listdir(cls_path):\n",
    "                shutil.copy(\n",
    "                    os.path.join(cls_path, img),\n",
    "                    os.path.join(TARGET_DIR, split, target_label, img)\n",
    "                )\n",
    "\n",
    "    print(\"‚úÖ Binary dataset created successfully!\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Dataset split skipped (script runned already and saved to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfba8ec1-577b-434c-8887-b340ab89abcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed: binary_dataset/.DS_Store\n",
      "Removed: binary_dataset/test/.DS_Store\n",
      "Removed: binary_dataset/train/.DS_Store\n",
      "Removed: binary_dataset/val/.DS_Store\n",
      "\n",
      "‚úÖ Total .DS_Store files removed: 4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def remove_ds_store(root_dir):\n",
    "    removed = 0\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        if \".DS_Store\" in files:\n",
    "            ds_path = os.path.join(root, \".DS_Store\")\n",
    "            os.remove(ds_path)\n",
    "            removed += 1\n",
    "            print(f\"Removed: {ds_path}\")\n",
    "    print(f\"\\n‚úÖ Total .DS_Store files removed: {removed}\")\n",
    "\n",
    "remove_ds_store(\"binary_dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6082474c-66c5-497b-b317-c774c6397ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN\n",
      "non_recyclable: 1365\n",
      "recyclable: 3833\n",
      "\n",
      "VAL\n",
      "non_recyclable: 292\n",
      "recyclable: 818\n",
      "\n",
      "TEST\n",
      "non_recyclable: 295\n",
      "recyclable: 828\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base = \"binary_dataset\"\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    print(f\"\\n{split.upper()}\")\n",
    "    for cls in os.listdir(os.path.join(base, split)):\n",
    "        count = len(os.listdir(os.path.join(base, split, cls)))\n",
    "        print(f\"{cls}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c018470-f74a-40b8-8aaf-9b9be6e0a404",
   "metadata": {},
   "source": [
    "## **multi class** - only recyclable dataset\n",
    "- ‚úî Reads from dataset/\n",
    "- ‚úî Copies only recyclable classes\n",
    "- ‚úî Preserves splits\n",
    "- ‚úî Safe to run once\n",
    "\n",
    "**üìÅ TARGET STRUCTURE**\n",
    "\n",
    "- recyclable_dataset/\n",
    "-    ‚îú‚îÄ‚îÄ train/\n",
    "-    ‚îÇ   ‚îú‚îÄ‚îÄ paper/\n",
    "-    ‚îÇ   ‚îú‚îÄ‚îÄ plastic/\n",
    "-    ‚îÇ   ‚îú‚îÄ‚îÄ glass/\n",
    "-    ‚îÇ   ‚îú‚îÄ‚îÄ metal/\n",
    "-    ‚îÇ   ‚îú‚îÄ‚îÄ cardboard/\n",
    "-    ‚îÇ   ‚îî‚îÄ‚îÄ e-waste/\n",
    "-    ‚îú‚îÄ‚îÄ val/\n",
    "-    ‚îî‚îÄ‚îÄ test/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a30da81-8872-41b6-8f9d-cabade8986fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è Dataset split skipped (script runned already and saved to disk\n"
     ]
    }
   ],
   "source": [
    "RUN_DATASET_SPLIT = False \n",
    "\n",
    "if RUN_DATASET_SPLIT:\n",
    "    import os\n",
    "    import shutil\n",
    "\n",
    "    SOURCE_DIR = \"dataset\"\n",
    "    TARGET_DIR = \"recyclable_dataset\"\n",
    "\n",
    "    recyclable_classes = [\n",
    "        \"paper\", \"plastic\", \"glass\",\n",
    "        \"metal\", \"cardboard\", \"e-waste\"\n",
    "    ]\n",
    "\n",
    "    splits = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "    # Create folders\n",
    "    for split in splits:\n",
    "        for cls in recyclable_classes:\n",
    "            os.makedirs(\n",
    "                os.path.join(TARGET_DIR, split, cls),\n",
    "                exist_ok=True\n",
    "            )\n",
    "\n",
    "    # Copy files\n",
    "    for split in splits:\n",
    "        split_path = os.path.join(SOURCE_DIR, split)\n",
    "\n",
    "        for cls in recyclable_classes:\n",
    "            cls_path = os.path.join(split_path, cls)\n",
    "            if not os.path.isdir(cls_path):\n",
    "                continue\n",
    "\n",
    "            for img in os.listdir(cls_path):\n",
    "                shutil.copy(\n",
    "                    os.path.join(cls_path, img),\n",
    "                    os.path.join(TARGET_DIR, split, cls, img)\n",
    "                )\n",
    "\n",
    "    print(\"‚úÖ Recyclable dataset created successfully!\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Dataset split skipped (script runned already and saved to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ca29e35-e18c-472f-a710-5a4ce6036c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed: recyclable_dataset/.DS_Store\n",
      "Removed: recyclable_dataset/val/.DS_Store\n",
      "\n",
      "‚úÖ Total .DS_Store files removed: 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def remove_ds_store(root_dir):\n",
    "    removed = 0\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        if \".DS_Store\" in files:\n",
    "            ds_path = os.path.join(root, \".DS_Store\")\n",
    "            os.remove(ds_path)\n",
    "            removed += 1\n",
    "            print(f\"Removed: {ds_path}\")\n",
    "    print(f\"\\n‚úÖ Total .DS_Store files removed: {removed}\")\n",
    "\n",
    "remove_ds_store(\"recyclable_dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fb41789-7a96-4183-b82d-8ce83e8c4205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN\n",
      "paper: 597\n",
      "metal: 630\n",
      "cardboard: 625\n",
      "e-waste: 695\n",
      "glass: 663\n",
      "plastic: 623\n",
      "\n",
      "VAL\n",
      "paper: 127\n",
      "metal: 135\n",
      "cardboard: 133\n",
      "e-waste: 148\n",
      "glass: 142\n",
      "plastic: 133\n",
      "\n",
      "TEST\n",
      "paper: 129\n",
      "metal: 136\n",
      "cardboard: 135\n",
      "e-waste: 150\n",
      "glass: 143\n",
      "plastic: 135\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base = \"recyclable_dataset\"\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    print(f\"\\n{split.upper()}\")\n",
    "    for cls in os.listdir(os.path.join(base, split)):\n",
    "        count = len(os.listdir(os.path.join(base, split, cls)))\n",
    "        print(f\"{cls}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f97411-c866-458b-a2f6-64021281b6b3",
   "metadata": {},
   "source": [
    "***all splits were preserved**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d07c00-89d4-4ba8-8410-2e2c22a9c004",
   "metadata": {},
   "source": [
    "## **summary** :\n",
    "The dataset was prepared to support a hierarchical waste classification pipeline consisting of a binary and a multi-class model.\n",
    "The original waste image dataset was first split into training (70%), validation (15%), and test (15%) sets in a class-wise manner to avoid data leakage.\n",
    "\n",
    "To train the binary classifier, waste categories were grouped into:\n",
    "- Recyclable: cardboard, paper, plastic, glass, metal, e-waste\n",
    "- Non-recyclable: organic, textile\n",
    "- Images were reorganized into a binary dataset while keeping the same train/validation/test split.\n",
    "\n",
    "To train the multi-class classifier, only recyclable waste images were used. A separate dataset containing six classes (cardboard, paper, plastic, glass, metal, e-waste) was created, excluding non-recyclable categories.\n",
    "\n",
    "\n",
    "\n",
    "***The dataset was split into train, validation, and test sets, then reorganized into binary and multi-class datasets to support a hierarchical waste classification approach.‚Äù***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763ffcf7-753c-438d-adb5-e2b144d8ffdd",
   "metadata": {},
   "source": [
    "## **Project Architecture**\n",
    "\n",
    "#### Input Image\n",
    "   ‚Üì\n",
    "#### Binary Classifier\n",
    "(Recyclable / Non-Recyclable)\n",
    "   \n",
    "   ‚Üì\n",
    "#### If Recyclable\n",
    "   \n",
    "   ‚Üì\n",
    "#### Multi-Class Classifier\n",
    "(Paper / Plastic / Glass / Metal / Cardboard / E-waste)\n",
    "\n",
    "**objectives :**\n",
    "\n",
    "- fristly we will train a CNN model on binary dataset that we created which classifies waste into recyclable/non-recyclable and save the trained model.\n",
    "- nextly we train another CNN model on multi class dataset to classify waste to type of recyclable waste.and save the model\n",
    "\n",
    "- now in deployment we will use these two models to support hierarchical waste classification approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
